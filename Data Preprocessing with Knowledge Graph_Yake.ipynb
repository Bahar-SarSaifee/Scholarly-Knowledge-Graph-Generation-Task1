{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480ea29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "# Import Dependencies & Load dataset\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3419a2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "\n",
    "data = pd.read_csv(\"task1_test_no_label.csv\", header=0, usecols=[0,1,3])\n",
    "\n",
    "# merge two columns of title and description to Content\n",
    "\n",
    "data['Content'] = data['title'] +\" \"+ data['description'].astype(str)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91559aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:40<00:00, 99.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Knowledge Graph\n",
    "\n",
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "      # check: token is a modifier or not\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "      ## chunk 3\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"      \n",
    "\n",
    "            ## chunk 4\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "\n",
    "    ## chunk 5  \n",
    "    # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "\n",
    "  #############################################################\n",
    "    return [ent2.strip(), ent1.strip()]\n",
    "\n",
    "entity_pairs = []\n",
    "\n",
    "counter = 0\n",
    "for i in tqdm(data['Content']):\n",
    "    entity_pairs.append([])\n",
    "    entity_pairs[counter].extend(get_entities(i))\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "data['Content'] = entity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba279f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove StopWords, words less 2 letter\n",
    "\n",
    "def remove_stp_less2letter(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    words = \" \". join([w for w in words if len(w)>2])\n",
    "    return words\n",
    "\n",
    "data['Content'] = data['Content'].apply(lambda x: remove_stp_less2letter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b9d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bahar/anaconda3/envs/thesis/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove http, html, digits\n",
    "\n",
    "def remove_http_html_digit_punc(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    html_free = soup.get_text()\n",
    "    no_http = re.sub(r\"http\\S+\", '', html_free)\n",
    "    no_digit= re.sub(r\"[0-9]\",\"\",no_http)\n",
    "    no_p = \"\". join([c for c in no_digit if c not in string.punctuation])\n",
    "    return no_p\n",
    "\n",
    "data['Content'] = data['Content'].apply(lambda x:remove_http_html_digit_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1137d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Preprocess_Test_KG.csv\")\n",
    "# data.to_csv(\"Preprocess_Train_KG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd40842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "\n",
    "data = pd.read_csv(\"task1_test_no_label.csv\", header=0, usecols=[0,1,3])\n",
    "\n",
    "# merge two columns of title and description to Content\n",
    "\n",
    "data['Content'] = data['title'] +\" \"+ data['description'].astype(str)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f29d903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Opsin-Based Photoreceptors, Birds, Photorecep...\n",
       "1       [Castleman disease, FDG-PET, multicentric Cast...\n",
       "2       [Neo, Assyrian, phonological difference, Babyl...\n",
       "3       [Family-focused cognitive, treatment trial, co...\n",
       "4       [evoked responses, information theoretic, theo...\n",
       "                              ...                        \n",
       "9995    [SANS studies, epithelial surfaces, mammalian ...\n",
       "9996    [anomalies Journal, leverage risk, Size, anoma...\n",
       "9997    [Creating, NIMBYs, urban development, generati...\n",
       "9998    [Problem structuring, intervention tools, mult...\n",
       "9999    [Strong interactions, massive gravity, exact s...\n",
       "Name: Content, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yake- Ky-Extraction\n",
    "\n",
    "import yake\n",
    "\n",
    "keywords_Yake = []\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 10\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "for i in range(len(data['Content'])):\n",
    "    keywords = kw_extractor.extract_keywords(data['Content'][i])\n",
    "    keywords_Yake.append([])\n",
    "    for kw in keywords:\n",
    "        keywords_Yake[i].append(kw[0])\n",
    "        \n",
    "\n",
    "# remove none value in list of keywords\n",
    "keywords_Yake_new = []\n",
    "for val in keywords_Yake:\n",
    "    if len(val) != 1 :\n",
    "        keywords_Yake_new.append(val)\n",
    "\n",
    "data['Content'] = keywords_Yake\n",
    "\n",
    "data['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5795aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove StopWords, words less 2 letter\n",
    "\n",
    "def remove_stp_less2letter(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    words = \" \". join([w for w in words if len(w)>2])\n",
    "    return words\n",
    "\n",
    "data['Content'] = data['Content'].apply(lambda x: remove_stp_less2letter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf69de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove http, html, digits\n",
    "\n",
    "def remove_http_html_digit_punc(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    html_free = soup.get_text()\n",
    "    no_http = re.sub(r\"http\\S+\", '', html_free)\n",
    "    no_digit= re.sub(r\"[0-9]\",\"\",no_http)\n",
    "    no_p = \"\". join([c for c in no_digit if c not in string.punctuation])\n",
    "    return no_p\n",
    "\n",
    "data['Content'] = data['Content'].apply(lambda x:remove_http_html_digit_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d148b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Preprocess_Test_Yake.csv\")\n",
    "# data.to_csv(\"Preprocess_Train_Yake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae050f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615de47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
